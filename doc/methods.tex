\section{Methods}

\subsection{Renewal model for incidence data} 

Effective reproduction number $\calR_t$, the expected secondary infection by a primary infection in a population at time $t$, is inherently a ratio of new infections at $t$ over the total primary infectious cases until $t$ (excluding the new infections). Here, we assume an ideal scenario of homogenous population that individuals follow similar social behaviors, exposure risks and random mixing patterns such as having similar contact rates to each other or similar susceptibility and infectiousness. 
Denote the new infections at time $t$ as $N_t$. Given an infectious period $\tau_t$ at $t$, we construct the total primary infectiousness as $\Lambda_t := \sum_{i=1}^{\tau_t} p_i N_{t-i}$, where $p_i$ is the infectious probability that a secondary case is infected by a primary case which is infected $i$ timepoints ago. The reproduction number can then be constructed based on the ratio $\calR_t := N_t / \Lambda_t$. We assume a constant observable proportion of infections $c\in (0,1)$, which is applied to all incidence counts and gets cancelled in $\calR_t$ ratio. 
Rearranging terms of the ratio yields the widely used renewal equation 
\begin{equation} \label{eq:renew-equation}
  N_t = \sum_{i=1}^{\tau_t} p_i\calR_t N_{t-i}
\end{equation}
for the analysis of transmission dynamics of infectious diseases. Some examples are EpiEstim \citep{cori2013new} and EpiFilter \citep{parag2021improved}. 

% serial interval probabilities
The sequence of probabilities $p_{1:\tau_t}$ gives the probabilities that a secondary infection at $t$ is infected by a primary infection that is infected $1$ to $\tau_t$ timepoints ago. The period between primary and secondary infections is called generation time. Theoretically, $p_{1:\tau_t}$ are the cumulative probabilities of generation time in discretized, contiguous time intervals, i.e., $(0,1], (1,2], \dots, (\tau_{t-1}, \tau_t]$. We assume that the infectiousness disappears beyond $\tau_t$ timepoints, so that the sequence $p_{1:\tau_t}$ has a sum of $1$. 

The generation time (period between the infections), however, is unobservable and tricky to estimate. We take a common strategy --- approximate it by serial interval (period between the onsets of symptoms). When the infectiousness profile after symptoms is independent of the incubation period, the serial interval is regarded identical as the generation time \citep{cori2013new}. 
We assume the distribution of generation time, and correspondingly serial interval, to be independent to time $t$, i.e., the probability $p_i$ only corresponds to the relative time range $i$ between a primary and secondary infections. The sequence $p_{1:\tau_t}$ only depends on $t$ by a chosen length $\tau_t$; if $\tau_t$ is given, the probability sequence will be right-truncated at $\tau_t$ and rescaled to have a sum of $1$. 
We choose Gamma distribution to estimate the serial interval, which is demonstrated to be a reasonable choice in practice. %\citep{cori2013new,thompson2019improved,abry2020spatial,pascal2022nonsmooth}


% advantage of using the renewal equation
The renewal equation in \eqref{eq:renew-equation} quantifies the transmission dynamic that primary incidences result in new incidences by effective reproduction numbers given serial interval. This dynamic is straightforward and efficient in data usage as it only depends on the observed incidence counts, which can be easily obtainable and usually sufficient and of good quality, and the specification of serial interval distribution. 

\subsection{Poisson trend filtering estimator} %Proximal optimization

We use the daily confirmed cases $y_t$ on day $t$ to estimate the observed infectious cases by assuming a consistent incubation period (from the time of infections to the time of symptom onsets when the cases are confirmed) and further assume $y_t$ to be Poisson distributed with mean $I_t$, i.e., 
\begin{equation*} 
  y_t \sim \mathrm{Poisson}(I_t) = \mathrm{Poisson}(\Lambda_t \calR_t).
\end{equation*}
Considering a fixed period of days $n$, our interest is to estimate the Poisson parameter $\calR_t$ given observations ${y}_{1:n} := \{y_1,\dots,y_n\}$ and the total infectiousness based on confirmed cases $\Lambda^{\ast}_t := \sum_{i=1}^{\tau_t} p_i y_{t-i}$. A natural approach is to solve the maximum likelihood estimates (MLEs), i.e., 
\begin{equation} \label{eq:mle}
  \hat{\calR}_t := \Argmax{\calR_t} \bbP(\calR_t \mid y_{1:n}, p_{1:\tau_t}) = \prod_{t = 1,\dots,n} \frac{e^{- \calR_t \Lambda^{\ast}_t} \lr{\calR_t \Lambda^{\ast}_t}^{y_t} }{y_t!}.
\end{equation}
This maximization problem, however, yields a one-to-one correspondence between the confirmed case (or the total infectiousness) and the effective reproduction number at each day, so that the estimated curves have no significant graphical smoothness. 

Smoothness of the effective reproduction numbers is a key to understand the trend of transmissibility of infectious diseases in retrospective studies. Smoother estimated curves give more high-level ideas with less changing points and hide the information of minor importance, and vice versa. 
We assume the effective reproduction numbers to be piecewise polynomials with multiple knots (i.e., changing points) with varying degrees. We specifically consider discrete splines with various degrees of continuity. For instance, the $0$th degree discrete splines are piecewise constant, the $1$st degree curves are piecewise linear, and the $2$nd degree curves are piecewise quadratic. For $k\geq 2$, the $k$th degree discrete splines are continuous and have continuous discrete differences up to degree $k-1$ at the knots. 

To achieve such smoothness, we regularize the distance between adjacent effective reproduction numbers. Since $\calR_t > 0$, penalizing the distance between $\calR_t$s directly may cause numerical issues such that there may be negative estimates in computation. Therefore, we equivalently penalize the distance between natural logarithm between neighboring $\calR_t$s through divided differences (i.e., discrete derivatives) with various orders.  
Compared to splines, discrete splines introduce computational efficiency. 
We penalize $\ell_1$ norm of the distance, which introduces sparsity into the curvature, so that the estimates have heterogeneous smoothness in different subregions of the entire domain. It is a more realistic setting compared to homogeneous smoothness in squared $\ell_2$ norm. 
The divided differences (i.e., discrete derivatives) with various orders to represent the temporal evolution of reproduction numbers with different degrees. 

We define a penalized regression problem to solve the MLE problem in \eqref{eq:mle} and the smoothness regularization simultaneously in \label{eq:rt-ptf}. It is a minimization problem with Poisson loss and the trend filtering penalty \citep{kim2009ell_1,tibshirani2014adaptive}. 
Let $\theta := \log(\calR) \in \bbR^n$, and then $\Lambda\circ \calR = \Lambda\circ e^{\theta}$, $\log(\Lambda\circ \calR) = \log(\Lambda) + \theta$, where $\circ$ is elementwise product, $e^{a}, \log(a)$ apply to a vector $a$ elementwise. 
The problem solves a Poisson trend filtering (PTF) estimator on univariate cases. For evenly spaced observed incidences, it is defined as: 
%We further regularize the smoothness of the reproduction number using the $\ell_1$ norm of the divided difference of the natural logarithm of $\calR$, which is real-valued. 
\begin{equation} \label{eq:rt-ptf}
    \begin{split}
        \hat{\theta} &:= \Argmin{\theta\in\bbR^n} \fr{n}\sumN -y_i \theta_i + \Lambda_i e^{\theta_i} + \lambda \norm{D^{(k+1)} \theta}_1, \\
        \hat{\calR} &:= e^{\hat{\theta}},
    \end{split}
\end{equation}
where $D^{k+1} \in \bbZ^{(n-k-1)\times n}$ is a $k$-th order divided difference matrix with $k = 0,1,2,\dots$. Define $D^{(k+1)}$ recursively as $D^{(k+1)} := D^{(1)} D^{(k)}$, where $D^{(1)} \in \bbN^{(n-k-1)\times (n-k)}$ is a banded matrix defined as
$$D^{(1)} := 
\begin{pmatrix}
-1 & 1 &  & & \\
 & -1 & 1 & & \\
 & & \vdots & \vdots & \\
 & & & -1 & 1
\end{pmatrix}.
$$
Define $D^{(0)} := I_n$, which is an identity matrix with size $n$. An exponential transformation is applied to the PTF estimator to get the estimated reproduction numbers. 

The tuning parameter $\lambda$ balances the contributions between data fidelity and smoothness. When $\lambda=0$, the problem in \eqref{eq:rt-ptf} reduces to regular least squares problem. A larger tuning parameter gives a higher importance on the regularization term and yields a smoother curve until the divided differences are all zeros, i.e., all parameters are projected onto the null space of the corresponding divided difference matrix. 

For unevenly spaced observations, the distances between neighboring parameters vary by the periods between observation times, and thus, the divided differences should be adjusted by observation days (or data locations). Given the data locations $x_{1:n} = \{x_1,\dots,x_n\}$, define a $k$th order diagonal matrix $$X^k := \diag \lr{\frac{k}{x_{k+1} - x_1}, \frac{k}{x_{k+2} - x_2}, \cdots, \frac{k}{x_n - x_{n-k}} }$$ for $k \geq 1$. Let $D^{(x,1)} := D^{(1)}$ and define $D^{(x,k+1)}$ for $k\geq 1$ recursively as $$D^{(x,k+1)} := D^{(1)}\cdot X^k \cdot D^{(x,k)}.$$ 

\cite{pascal2022nonsmooth} considered the second-order divided different of effective reproduction number. In comparison to their study, our estimator is more flexible in the degree of temporal evolution of the effective reproduction numbers and also avoid the potential numerical issues of penalizing/estimating positive numbers. 
Our estimator is locally adaptive so that it captures the local changes such as the initiation of effective control measures. More specifically, it regularizes the similarity among reproduction numbers across a chosen number of neighboring time points and segments the curvature of the reproduction numbers such that there are more jumpiness in some subregions and more smoothness in others. 

\subsection{Proximal Newton solver} %Specialized ADMM for `generalized' Poisson trend filtering on lines

The proximal Newton method is a second-order algorithm solving a proximal Newton optimization iteratively followed by a line search algorithm adjusting the step size at each iteration for faster convergence. The proximal Newton method for Poisson trend filtering in \eqref{eq:rt-ptf} takes a second-order Taylor expansion of the Poisson loss, which results in a proximal optimization --- Gaussian trend filtering with dynamic weights during iteration. 

Let $g(\theta):= \fr{n} \sumN -y_i\theta_i + \Lambda_i e^{\theta_i}$ be the Poisson regression loss and $h(\theta) := \lambda \norm{D^{(k+1)} \theta}_1$ be the regularization. At iterate $t+1$, consider the following approximation of $g(\theta)$ using a second-order Taylor expansion around $\theta^t$: 
$$ g(\theta) = g(\theta^t) + (\theta - \theta^t)^{\top} \nabla^{(1)}_{\theta} g(\theta^t) + \fr{2} (\theta - \theta^t)^{\top} \nabla^{(2)}_{\theta} g(\theta^t) (\theta - \theta^t), $$
where $\nabla^{(1)}_{\theta} g(\theta^t) = \fr{n} \lr{-y + \Lambda\circ e^{\theta^t}} \in \bbR^n$ is the gradient of $g(\theta)$ at $\theta^t$ and $\nabla^{(2)}_{\theta} g(\theta^t) = \fr{n}\diag \lr{\Lambda \circ e^{\theta^t}} \in \bbR^{n\times n}$ is the Hessian matrix of $g(\theta)$ at $\theta^t$. %The gradient of $g(\theta)$ then can be approximated by $$\nabla_{\theta} g(\theta) := \nabla_{\theta} g(\theta^t) + \nabla^2_{\theta} g(\theta^t) (\theta - \theta^t) = \fr{n} \lr{W^t \theta - c^t} = \fr{n}W^t \lr{\theta - {c^t}^{\ast}},$$ where $c^t := y-e^{\theta^t}+\theta^t\circ e^{\theta^t}$, $W^t = \mathrm{diag}\lr{e^{\theta^t}}$, and ${c^t}^{\ast} = y\circ e^{-\theta^t} - \boldsymbol{1} + \theta^t$ given $e^{\theta_i^t} \in \bbR_{++}, i=1,...,n$. 

Define the proximal operator as $\mathrm{prox}_{W,D} (x) := \Argmin{z\in\bbR^n} \fr{2n} \norm{z-x}_W^2 + \lambda \norm{D\theta}_1$, where $\norm{a}_{W}^2 := a^{\top} {W} a$. The proximal optimization problem at iterate $t+1$ can be further written as, given $\theta^t$,
\begin{equation} \label{eq:prox-gauss}
    \begin{split}
        \theta^{t_+} :&= \Argmin{\theta\in\bbR^n} (\theta - \theta^t)^{\top} \nabla^{(1)}_{\theta} g(\theta^t) + \fr{2} (\theta - \theta^t)^{\top} \nabla^{(2)}_{\theta} g(\theta^t) (\theta - \theta^t) + h(\theta), \\
        &= \Argmin{\theta\in\bbR^n} \fr{2n} \norm{\theta - c^{t}}_{W^t}^2 + \lambda \norm{D^{(k+1)}\theta}_1, \\
        &= \mathrm{prox}_{W^t,D^{(k+1)}} (c^{t}),
    \end{split}
\end{equation}
where $W^t := \diag \lr{\Lambda\circ e^{\theta^t}}$ is the weighted (Hessian) matrix multiplied by $n$ and ${c^t} := \theta^t - \lr{W^{t}}^{-1} \lr{n\nabla^{(1)}_{\theta} g(\theta^t)} = y\circ \Lambda^{-1}\circ e^{-\theta^t} - \boldsymbol{1} + \theta^t\circ \Lambda^{-1}$, where $\{e^{\theta^t}\}_{i\in[n]} > 0, [n]:=1,2,\dots,n$.
This is just univariate \textit{Gaussian trend filtering} with weights $W^t$ \citep{tibshirani2014adaptive}. 

We solve the trend filtering problem in \eqref{eq:prox-gauss} using the specialized ADMM, proposed by \cite{ramdas2016fast}, with the primal $\theta$ step solved in closed-form and the auxiliary step solved by the dynamic programming algorithm for fused lasso proposed by \cite{johnson2013dynamic}. Let the auxiliary variable $z:= D^{(k)}\theta$. The scaled augmented Lagrangian is $$\mathcal{L}_{\lambda, \rho}(\theta, z, u) = \fr{2n} \norm{\theta - c^{t}}_{W^t}^2 + \lambda \norm{D^{(1)}z}_1 + \frac{\rho}{2} \norm{D^{(k)}\theta - z + u}^2 - \frac{\rho}{2} \norm{u}^2, $$ where $\rho$ is a scaled dual parameter and $u$ is a dual variable. The specialized ADMM solves the following subproblems, at ADMM iteration $l+1$: 
\begin{equation}
  \begin{split}
      \theta^{l+1} &:= \Argmin{\theta} \fr{2n} \norm{\theta - c^{t}}_{W^t}^2 + \frac{\rho}{2} \norm{D^{(k+1)} \theta - z^l + u^l}_2^2, \\
      z^{l+1} &:= \Argmin{z} \frac{\lambda}{\rho} \norm{D^{(1)} z}_1 + \fr{2} \norm{D^{(k+1)} \theta^{l+1} - z + u^l}_2^2, \\
      u^{l+1} &\leftarrow u^l + D^{(k+1)} \theta^{l+1} - z^{l+1}.
  \end{split}
\end{equation}

We further adjust the step size $\gamma^t \in (0,1]$ at iterate $t$ by a backtracking line search algorithm: 
$$\theta^{t+1} \leftarrow \theta^t + \gamma^{t+1} (\theta^{t_+} - \theta^t).$$ The proximal Newton algorithm iterates until the convergence of objective.

% defer to the section of Simulation: 
%Range of $\lambda$: Find the maximum lambda such that the estimated $\theta$ does not fall into the null space of the divided difference matrix.
%% other computational considerations: low quality data (missingness, outliers, seasonalities, ...)? 

\subsection{Bayesian perspective}
%The epidemiology evolution mechanism suggests a hierarchical framework that posteriori of reproduction number depends on its prior and the distribution of confirmed cases. 

Since many current approaches are Bayesian methods, we here provide an alternative interpretation of our approach from the Bayesian perspective. Our approach can be interpreted as a state-space model of Poisson observational noises and Laplace transition noises with certain degree $k\geq 0$, e.g., $\theta_{t+1} = 2\theta_t - \theta_{t-1} + \varepsilon_{t+1}$ with $\varepsilon_{t+1}\sim \mathrm{Laplace}(0,1/\lambda)$ for $k=1$. Compared to EpiFilter \citep{parag2021improved}, another retrospective study of $\calR_t$, we share same observational assumptions, but our approach has a different transition noises. 
EpiFilter estimates the posterior distribution of $\calR_t$, and thus it can provide the credible interval estimation with various credible levels. Our approach solves the point estimation using optimization problem, which has the advantage of computational efficiency. 
