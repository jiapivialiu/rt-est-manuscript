\section{Methods}

\subsection{Renewal model for incidence data} 

The effective reproduction number $\calR(t)$
is defined to be the expected number of secondary infections at time $t$
produced by a primary infection sometime in the past.
To make this precise, denote the number
of new infections at time $t$ as $y(t)$. Then the total primary
infectiousness can be written as $\eta(t) := \int_0^{\infty} p(i) y(t-i)
\diff i$,
where $p(i)$ is the probability that a new secondary infection is the result
primary infection which occurred $i$ time units in the past. 
The reproduction number is
then given as the value that equates
\begin{equation} \label{eq:pre-renew-equation}
  \bbE[y(t) \mid y(j),\ j<t] = \calR(t)\eta(t) = \calR(t)\int_0^\infty p(i)y(t-i)\diff i,
\end{equation}
otherwise known as the renewal equation. 
The period between primary and secondary
infections is exactly the generation time of the disease, but given real data,
observed at discrete times (say, daily) this delay distribution must be discretized
into contiguous time intervals,
say, $(0,1], (1,2], \dots$, which results in the sequence $\{p_i\}_1^\infty$
corresponding to observations $y_t$ and resulting in the
discretized version of \eqref{eq:pre-renew-equation},
\begin{equation} \label{eq:renew-equation}
  \bbE[y_t \mid y_1,\ldots,y_{t-1}] = \calR_t\eta_t = \calR_t\sum_{i = 0}^\infty p_i y_{t-i}.
\end{equation}
Many approaches to estimating $\calR_t$ rely on \eqref{eq:renew-equation} as
motivation for their procedures, among them,
\EpiEstim\ \citep{cori2013new} and \texttt{EpiFilter}
\citep{parag2021improved}. 

% serial interval probabilities
In most cases, it is safe to assume that
infectiousness disappears beyond $\tau$ timepoints ($p(i) = 0$ for $i > \tau$)
so that the truncated integral of the generation interval distribution
$\int_0^\tau p(i)\diff i = 1$.
Generation time, however, is usually unobservable and tricky to estimate, so
common practice is to approximate it by the serial interval: the period between
the symptom onsets of primary and secondary infections. If the infectiousness
profile after symptom onset is independent of the incubation period (the period
from the time of infection to the time of symptom onset), then this
approximation is justifiable: the serial interval distribution and the
generation interval distribution share the same mean. However, other properties
may not be similarly shared, and, in general, the generation interval
distribution is a convolution of the serial interval distribution with the
distribution of the differance between independent draws from the delay
distribution from infection to symptom onset. See, for example,
\citet{gostic2020practical} for a fuller discussion of the dangers of this
approximation. Nonetheless, treating these as interchangable is common
\citep{cori2013new} and beyond the scope of this work. Additionally, we assume
that the generation interval (and, therefore, the serial interval), is constant over time
$t$. That is, the probability $p(i)$ depends only on the gap between primary and
secondary infections and not on the time $t$ when the secondary infection
occurs. For our methods, we will assume that the serial interval can be
accurately estimated from auxilliary data (say by contact tracing, or previous
epidemics) and we will take it as fixed, as is common in existing studies, e.g.,
\cite{cori2013new,abry2020spatial,pascal2022nonsmooth}.

% advantage of using the renewal equation
The renewal equation in \eqref{eq:renew-equation} relates observable data
streams (incident cases) occurring at different time points to the reproduction
number given the serial interval. The fact that it depends only on the observed
incidence counts makes it reasonable to estimate $\calR_t$. However, this
relationship obscures some difficulties in data collection. Diagnostic testing
targets symptomatic individuals, omitting asymptomatic primary infections which
can lead to future secondary infections. Testing practices, availability, and
uptake can vary across space and time \citep{pitzer2021impact,
hitchings2021usefulness}. Finally, incident cases as reported to public health
are subject to delays due to laboratory confirmation, test turnaround times, and
eventual submission to public health \citep{pellis2021challenges}. For these
reasons, reported cases are lagging indicators of the course of the pandemic.
Furthermore, they do not represent the actual number of new infections that
occur on a given day, as indicated by exposure to the pathogen. The assumptions
described above (constant serial interval distribution, homogenous mixing,
similar susceptibility and social behaviours, etc.) are therefore consequential.
That said, \eqref{eq:renew-equation} also provides some comfort about deviations
from these assumptions. If $y_t$ is scaled by a constant (in time) describing
the reporting ratio, then it will cancel from both sides. Similar arguments mean
that even if such a scaling varies in time, as long as it varies slowly relative
to the set of $p_i$ that are larger than 0, \eqref{eq:renew-equation} will be a
reasonably accurate approximation, so that $\calR_t$ can still be estimated well
from reported incidence data. Finally, even a sudden change, say from $c_1$ for
$i=1,\ldots,t_1$ to $c_2$ for $i>t_1$ would only result in large errors for $t$
in the neighbourhood of $t_1$ (where the size of this neighbourhood is again
determined by the effective support of $\{p_i\}$). This robustness to certain
types of data reporting issues provides some degree of comfort when depending on
\eqref{eq:renew-equation} to calculate $\calR_t$.

\subsection{Poisson trend filtering estimator} %Proximal optimization

We use the daily confirmed incident cases $y_t$ on day $t$ to estimate the
observed infectious cases under the model that $y_t$ given previous incident
cases $y_{t-1},\ldots,y_1$ and a constant serial interval distribution follows a
Poisson distribution with mean $\Lambda_t$. That is, 
$$
y_t \mid y_1,\ldots,y_{t-1} \sim \mathrm{Poisson}(\Lambda_t), \textrm{ where } \Lambda_t =  \calR_t\sum_{i=0}^{t-1}p_i y_{t-i} = \calR_t\eta_t.$$ 
Given a history of $n$ confirmed incidence counts $\bfy = (y_1,\ldots,y_n)^\top$,
our interest is to estimate $\calR_t$. A natural approach is to maximize the
likelihood, producing the MLE:
\begin{equation} \label{eq:mle}
  \begin{split}
    \widehat{\calR} &= \Argmax{\calR \in \bbR_+^n} \bbP(\calR \mid \bfy,\ \bfp)
    = \Argmax{\calR \in \bbR^n_+} \prod_{t = 1,\dots,n} 
    \frac{e^{- \calR_t \eta_t} \lr{\calR_t \eta_t}^{y_t} }{y_t!}\\
    &= \Argmin{\calR\in\bbR^n_+} \frac{1}{n}\sum_{t = 1}^n \calR_t\eta_t - y_t\log(\calR_t\eta_t).
  \end{split}
\end{equation}
This optimization problem, however, is easily seen to yield a one-to-one
correspondence between the confirmed cases and the effective reproduction, i.e.,
$\widehat{\calR}_t = y_t / \eta_t$, so that the estimated sequence
$\widehat{\calR}$ will have no significant graphical smoothness.

The MLE is an unbiased estimator of the true parameter $\calR_t$, but
unfortunately has
high variance: changes in $y_t$ result in proportional changes in
$\widehat\calR_t$. To avoid this behaviour, and to match the intuition that
$\calR_t \approx \calR_{t-1}$, we advocate enforcing smoothness of the effective
reproduction numbers. This requirement will decrease the variance, and hopefully
lead to more
accurate estimation of $\calR$, as long as the smoothness assumption is
reasonable. Smoothness assumptions are common (see e.g.,
\citet{parag2021improved} or \citet{gostic2020practical}), but the \emph{type}
of smoothness assumed is critical. \citet{cori2020package} imposes smoothness
indirectly by estimating $\calR_t$ with moving windows of of past observations.
The Kalman filter procedure of \citet{parag2021improved} would result in 
$\ell_2$-smoothness ($\int_0^n (\widehat{\calR}''(t))^{2}\diff t < C$ for some $C$),
although the algorithm results in $\widehat{\calR}$ taking values
over a discrete grid. \citet{pascal2022nonsmooth} produces
piecewise-linear $\widehat{\calR}_t$, which turns out to be a special case of
our methodology.
%Meanwhile, the smoothed estimation can still keep the critical changing points
%of the transmissibility for the reference to policy makers. 
%Smoothness of the effective reproduction numbers is a key to understand the trend of transmissibility of infectious diseases in retrospective studies. 
Smoother estimated curves will provide high-level information about the
entire epidemic, obscuring small local changes in $\calR(t)$, but may also remove
the ability to detect large sudden changes, such as those resulting from
lockdowns or other major containment policies. 

We choose to implement smoothness by assuming that $\calR(t)$ is a piecewise
polynomial of arbitrary degree. We specifically consider discrete splines with
various degrees of continuity. For example, $0\th$-degree discrete splines are
piecewise constant, the \first-degree curves are piecewise linear, and
\second-degree curves are piecewise quadratic. For $k\geq 1$, $k\th$-degree
discrete splines are continuous and have continuous discrete differences up to
degree $k-1$ at the knots. To achieve such smoothness, we regularize the size of
changes between adjacent effective reproduction numbers. Because $\calR_t > 0$,
we explicitly penalize the divided differences (discrete derivatives) of
neighbouring values of $\log(\calR)_t$. To achieve this, we penalize the
$\ell_1$ norm of the divided differences, which introduces sparsity into the
curvature, so that the estimates have heterogeneous smoothness in different
subregions of the entire domain. It is a more realistic setting compared to
homogeneous smoothness created by the squared $\ell_2$ norm. Taking different
orders of divided differences result in estimated effective reproduction numbers
with different smoothness assumptions. 

To enforce smoothness of $\hat\calR_t$, we add a trend filtering penalty to
\eqref{eq:rt-ptf}
\citep{kim2009ell_1,tibshirani2014adaptive,tibshirani2022divided,sadhanala2022exponential}.
Let $\theta := \log(\calR) \in \bbR^n$, so that $\Lambda_t =
\eta_t \exp(\theta_t)$, and $\log(\eta_t \calR_t) = \log(\eta_t) +
\theta_t$. For evenly spaced incident case data, we
write our estimator as the solution to the optimization problem
%We further regularize the smoothness of the reproduction number using the $\ell_1$ norm of the divided difference of the natural logarithm of $\calR$, which is real-valued. 
\begin{equation} 
  \label{eq:rt-ptf}
  \widehat{\calR} = \exp(\widehat{\theta}) \quad\textrm{where}\quad \widehat{\theta} = \Argmin{\theta\in\bbR^n} \eta^\top \exp(\theta) - \bfy^\top \theta + \lambda \norm{D^{(k+1)} \theta}_1,
\end{equation}
where $\exp(\cdot)$ applies elementwise.
Here, $D^{(k+1)} \in \bbZ^{(n-k-1)\times n}$ is the $(k+1)\th$ order divided
difference matrix for any $0\leq k < n-1$. $D^{(k+1)}$ is defined recursively as
$D^{(k+1)} = D^{(1)} D^{(k)}$, where $D^{(1)} \in \{-1,0,1\}^{(n-k-1)\times
(n-k)}$ is a banded matrix with diagonal band: 
$$D^{(1)} = \begin{pmatrix} 
  -1 & 1 &  & & \\ 
  & -1 & 1 & & \\ 
  & & \ddots & \ddots & \\
  & & & -1 & 1 
\end{pmatrix}.$$ 
The tuning parameter $\lambda$ balances data
fidelity with desired smoothness. When $\lambda=0$, the problem in
\eqref{eq:rt-ptf} reduces to the MLE in \eqref{eq:mle}. Larger tuning parameters
privilege the regularization term and yield smoother estimates. Finally, there
exists $\lambda_{\textrm{max}}$ such that any $\lambda \geq
\lambda_{\textrm{max}}$ will result in $D^{(k+1)} \widehat {\theta} = 0$ and
$\widehat{\theta}$ will be the Kullback-Leibler projection of $\bfy$ onto the
null space of $D^{(k+1)}$.

For unevenly spaced incidence data, the spacing between neighboring parameters
varies by the time between observations, and thus, the divided differences must
be adjusted by the times that the observations occur.
Given observation times $\bfx = (x_1,\dots,x_n)^\top$, for $k \geq 1$, define a
$k\th$-order diagonal matrix $$X^{(k)} = \diag \lr{\frac{k}{x_{k+1} - x_1},\ 
\frac{k}{x_{k+2} - x_2},\ \cdots,\ \frac{k}{x_n - x_{n-k}} }.$$ 
Let $D^{(\bfx,1)} :=
D^{(1)}$. Then for $k\geq 1$, the $(k+1)\th$-order divided difference matrix
for unevenly spaced data can be created recursively by
$$D^{(\bfx, k+1)} := D^{(1)} X^{(k)} D^{(\bfx,\ k)}.$$ 

Importantly, due to the penalty structure, this estimator is locally adaptive,
meaning that it can potentially capture local changes such as the initiation of
control measures. \cite{abry2020spatial,pascal2022nonsmooth} considered only the
\second-order divided difference of $\calR_t$ rather than its logarithm. In
comparison to their work, our estimator (1) allows for arbitrary degrees of
temporal smoothness and (2) avoids the potential numerical issues of
penalizing/estimating positive real values. Furthermore, as we will describe
below, our procedure is computationally efficient for estimation over an entire
sequence of penalty strengths $\lambda$ and provides methods for choosing how
smooth the final estimate should be.

\subsection{Numerical optimization of the $\calR_t$ estimator}

\attn{To appendix? Almost certainly.}

The proximal Newton method is a second-order algorithm solving a proximal optimization iteratively followed by a line search algorithm adjusting the step size at each iteration for faster convergence. The proximal Newton method for Poisson trend filtering in \eqref{eq:rt-ptf} solves an approximate problem iteratively --- specifically, it takes a second-order Taylor expansion of the Poisson loss, which results in a proximal optimization, i.e., trend filtering with squared $\ell_2$ loss, with dynamic weights during iteration, and solves it iteratively until convergence to the objective. 

Let $g(\theta)= \eta^\top \exp(\theta)-\bfy^\top \theta$ be the Poisson loss and
$h(\theta) = \lambda \Vert D^{(k+1)} \theta\Vert_1$ be the regularization in
\eqref{eq:rt-ptf}. At iterate $j+1$, consider the following approximation of
$g(\theta)$ using the second-order Taylor expansion around $\theta^j$, 
$$g(\theta) = g(\theta^j) + (\theta - \theta^j)^{\top} \nabla^{(1)}_{\theta}
g(\theta^j) + \fr{2} (\theta - \theta^j)^{\top} \nabla^{(2)}_{\theta}
g(\theta^j) (\theta - \theta^j), $$ where $\nabla^{(1)}_{\theta} g(\theta^j) =
\fr{n} \eta^\top \exp(\theta^j) - y \in \bbR^n$ is the gradient of $g(\theta)$
evaluated at $\theta^j$, and $\nabla^{(2)}_{\theta} g(\theta^j) = \fr{n}\diag
\lr{\eta \circ \exp(\theta^j)} \in \bbR^{n\times n}$ is the Hessian matrix of
$g(\theta)$ evaluated at $\theta^j$ and $\circ$ means elementwise product.
%The gradient of $g(\theta)$ then can be approximated by $$\nabla_{\theta} g(\theta) := \nabla_{\theta} g(\theta^t) + \nabla^2_{\theta} g(\theta^t) (\theta - \theta^t) = \fr{n} \lr{W^t \theta - c^t} = \fr{n}W^t \lr{\theta - {c^t}^{\ast}},$$ where $c^t := y-e^{\theta^t}+\theta^t\circ e^{\theta^t}$, $W^t = \mathrm{diag}\lr{e^{\theta^t}}$, and ${c^t}^{\ast} = y\circ e^{-\theta^t} - \boldsymbol{1} + \theta^t$ given $e^{\theta_i^t} \in \bbR_{++}, i=1,...,n$. 

Define the proximal operator as $\mathrm{prox}_{W,D} (\bfx) :=
\Argmin{\bfz\in\bbR^n} \fr{2n} \norm{\bfz-\bfx}_W^2 + \lambda \norm{D\theta}_1$,
where $\norm{\mathbf{a}}_{W}^2 := \mathbf{a}^{\top} {W} \mathbf{a}$. The
proximal optimization problem at iterate $j+1$ given $\theta^j$ becomes
\begin{equation} \label{eq:prox-gauss}
    \begin{split}
        \theta^{j_+} :&= \Argmin{\theta\in\bbR^n} (\theta - \theta^j)^{\top} \nabla^{(1)}_{\theta} g(\theta^j) + \fr{2} (\theta - \theta^j)^{\top} \nabla^{(2)}_{\theta} g(\theta^j) (\theta - \theta^j) + h(\theta), \\
        &= \Argmin{\theta\in\bbR^n} \fr{2n} \norm{\theta - \mathbf{c}^{j}}_{W^j}^2 + \lambda \norm{D^{(k+1)}\theta}_1, \\
        &= \mathrm{prox}_{W^j,D^{(k+1)}} (\mathbf{c}^{j}),
    \end{split}
\end{equation}
where $W^j := \diag \lr{\eta\circ\exp(\theta^j)}$ is the weighted (Hessian)
matrix multiplied by $n$ and $$\mathbf{c}^j := \theta^j - n \lr{W^{j}}^{-1}
\nabla^{(1)}_{\theta} g(\theta^j) = \bfy \circ \eta^{-1} \circ \exp(-\theta^j) -
\boldsymbol{1} + \theta^j \circ \eta^{-1}.$$
This is just univariate Gaussian trend filtering with weights $W^t$ \citep{tibshirani2014adaptive}. 

We solve the trend filtering problem in \eqref{eq:prox-gauss} using the
specialized ADMM, proposed by \cite{ramdas2016fast}, with the primal $\theta$
step solved in closed-form and the auxiliary step solved by the dynamic
programming algorithm for fused lasso proposed by \cite{johnson2013dynamic}. Let
the auxiliary variable $\bfz:= D^{(k)}\theta$. The scaled augmented Lagrangian
is $$\mathcal{L}_{\lambda, \rho}(\theta, \bfz, \mathbf{u}) = \fr{2n}
\norm{\theta - \mathbf{c}^{j}}_{W^j}^2 + \lambda \norm{D^{(1)}\bfz}_1 +
\frac{\rho}{2} \norm{D^{(k)}\theta - \bfz + \mathbf{u}}^2 - \frac{\rho}{2}
\norm{\mathbf{u}}^2, $$ where $\rho$ is a scaled dual parameter and $\mathbf{u}$
is the dual variable. At the $(j+1)\th$ Newton step, the specialized ADMM
solves the following subproblems, at ADMM iteration $l+1$: 
\begin{equation}
  \begin{split}
    \theta^{l+1} &:= \Argmin{\theta} \fr{2n} \norm{\theta - \mathbf{c}^{l}}_{W^j}^2 + \frac{\rho}{2} \norm{D^{(k+1)} \theta - \bfz^l + \mathbf{u}^l}_2^2, \\
    \bfz^{l+1} &:= \Argmin{\bfz} \frac{\lambda}{\rho} \norm{D^{(1)} \bfz}_1 +
    \fr{2} \norm{D^{(k+1)} \theta^{l+1} - \bfz + \mathbf{u}^l}_2^2, \\
    \mathbf{u}^{l+1} &\leftarrow \mathbf{u}^l + D^{(k+1)} \theta^{l+1} - \bfz^{l+1}.
  \end{split}
\end{equation}

Finally, the step size $\gamma^{j+1} \in (0,1]$ at iterate $j+1$ is adjusted by a backtracking line search algorithm to solve for $\theta^{j+1}$, i.e.,   
$$\theta^{j+1} \leftarrow \theta^j + \gamma^{j+1} (\theta^{j_+} - \theta^j).$$ The proximal Newton algorithm iterates until convergence of the objective.

% defer to the section of Simulation: 
%Range of $\lambda$: Find the maximum lambda such that the estimated $\theta$ does not fall into the null space of the divided difference matrix.
%% other computational considerations: low quality data (missingness, outliers, seasonalities, ...)? 


\subsection{Solving over a sequence of tuning parameters}

\attn{Need a section about the solving for a sequence of $\lambda$.}

\subsection{Choosing a final $\lambda$}

\attn{And one about CV. Mention  that other procedures don't choose this.}

\subsection{Approximate confidence bands}

\attn{And one about confidence bands.}

\subsection{Bayesian perspective}
%The epidemiology evolution mechanism suggests a hierarchical framework that posteriori of reproduction number depends on its prior and the distribution of confirmed cases. 

%We here provide an alternative interpretation of our approach from the Bayesian perspective. 
Unlike many other methods for $\calR_t$ estimation, our apporach is frequentist
rather than Bayesian. Nonetheless, it can has a corresponding Bayesian
interpretation: as a state-space model with Poisson observational noise,
autoregressive transition equation of degree $k\geq 0$, e.g., $\theta_{t+1} =
2\theta_t - \theta_{t-1} + \varepsilon_{t+1}$ for $k=1$, and Laplace transition
noise $\varepsilon_{t+1}\sim \mathrm{Laplace}(0,\ 1/\lambda)$. Compared to
\texttt{EpiFilter} \citep{parag2021improved}, another retrospective study of
$\calR_t$, we share same observational assumptions, but our approach has a
different transition noise. \texttt{EpiFilter} estimates the posterior
distribution of
$\calR_t$, and thus it can provide credible interval estimates as well. Our
approach produces the maximum \emph{a posteriori} estimate via an efficient
convex optimization, omitting the need for MCMC sampling. But the associated
confidence bands are approximated differently.

