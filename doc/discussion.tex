\section{Discussion}

The \RtEstim\ methodology provides a locally adaptive estimator using Poisson
trend filtering on univariate data. It captures the heterogeneous smoothness of
effective reproduction numbers given observed incidence data rather than
resulting in global smoothness. This is a nonparametric regression model which
can be written as a convex optimization (minimization) problem. Minimizing the
distance (averaged KL divergence per coordinate) between the estimators and
(functions of) observations guarantees data fidelity while the penalty on divided
differences between pairs of neighbouring parameters imposes smoothness. The
$\ell_1$-regularization results in sparsity of the divided differences, which
leads to heterogeneous smoothness across time. 


The property of local adaptivity (heterogenous smoothness) is useful to
automatically distinguish, for example, seasonal outbreaks from outbreaks driven
by other factors (behavioural changes, foreign introduction, etc.). Given a
well-chosen polynomial degree, the growth rates can be quickly detected, 
potentially advising public health to implement policy changes. The effective
reproduction numbers can be estimated retrospectively to examine the efficacy of
such policies, whether they result in $\calR_t$ falling below 1 or the speed of
their effects. The smoothness of $\calR_t$ curves (including the polynomial 
degrees and tuning parameters) should be chosen based on the purpose of the 
study in practice, e.g., epidemic forecasting may require less smoothness
while retrospective studies that 
solely target understanding of the pandemic may prefer a smoother estimate. 


Our method \RtEstim\ provides a natural way to deal with missing data, for
example, on weekends and holidays or due to changes in reporting frequency.
While solving the convex optimization problem, our method can be easily 
handle uneven spacing or irregular reporting. Computing the total
primary infectiousness is also easily generalized to irregular reporting by
modifying the discretization of the serial interval distribution. Additionally,
because the $\ell_1$ penalty introduces sparsity (operating like a median
rather than a mean), this procedure is relatively insensitive to outliers
compared to $\ell_2$ regularization.


There are a number of limitations that may influence the quality of
$\calR_t$ estimation. While our model is generic for incidence data, 
rather than tailored to any specific disease, it does assume that the 
generation interval is short relative to the period of data collection. 
More specialized methodologies would be required for diseases with long 
incubation periods such as HIV or Hepatitis. 
Our approach, does not explicitly model imported cases, nor distinguish between
subpopulations that may have different mixing behaviour. 
While the Poisson assumption is common, it does not handle overdispersion
(observation variance larger than the mean). The negative binomial distribution
is a good alternative, but more difficult to estimate in this context.
As described in \autoref{sec:intro}, the expression for $\calR$ 
assumes that a relatively constant proportion of true infections is reported. 
However, if this proportion varies with time (say, due to changes in surveillance
practices or testing recommendations), the estimates may be biased over this
window. A good example is in early January 2022, during the height of the
Omicron wave, British Columbia moved from testing all symptomatic individuals to
testing only those in at-risk groups. The result was a sudden change that would
render $\calR_t$ estimates on either side of this time point incommensurable.


As currently implemented, \RtEstim\ uses a fixed serial interval throughout the
period of study, but as factors such as population immunity vary, the serial
interval may vary as well \citep{nash2023estimating}.  
Another issue relates to equating serial and generation intervals (also
mentioned above). The serial interval distribution is generally wider than that
of the generation interval, because the serial interval involves the convolution
of two distributions, and is unlikely to actually follow a named distribution
like Gamma, though it may be reasonably well approximated by one. Our
implementation allows for an arbitrary distribution to be used, but requires the
user to specify the discretization explicitly, requiring more nuanced knowledge
than is typically available. Pushing this analysis further, to accommodate other
types of incidence data (hospitalizations or deaths), a modified generation
interval distribution would be necessary, and further assumptions would be
required as well. Or else, one would first need to deconvolve deaths to
infection onset before using our software.


Nonetheless, our \RtEstim\ estimator can be implemented easily through a
lightweight \R\ package \texttt{rtestim} and computed efficiently using the
included proximal Newton method solver coded in \cpp, especially for large-scale
data. Given available incident case data, prespecified serial interval
distribution, and a choice of degree $k$, \RtEstim\ is able to produce
accurate estimates of effective reproduction number and provide efficient
tuning parameter selection via cross validation. 
